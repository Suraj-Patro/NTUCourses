% !TEX program = xelatex
\documentclass{article}
\usepackage{/Users/jay/LaTeX/cs}
\usepackage{/Users/jay/LaTeX/xeCJK}

\newcommand{\hmwkClass}{Probability and Statistics, Spring 2018}
\newcommand{\hmwkTitle}{Homework 8}
\newcommand{\hmwkDueDate}{June 18, 2018}
\newcommand{\tb}{\textbf}

\begin{document}

\thispagestyle{empty}
\section*{\hmwkClass \\
    \normalsize{\hmwkTitle} \\
    \normalsize{DUE DATE: \hmwkDueDate}
}

\hfill{B03902129 \, 資工四 \, 陳鵬宇}

\begin{enumerate}
    \item [8.2.1] 
    
    \begin{align*}
    \text E[X_1X_2 \cdots X_i \dots X_n] 
        & = \text E[X_1] \text E[X_2] \cdots \text E[X_i] \cdot \text E[X_n] \\
        & = 0
    \end{align*}

    Thus, the $i$, $j^{\text{th}}$ entry in the convariance matric $C_X$ is defined as,

    $$C_X(i, j) = \begin{cases}
        \sigma_i^2 & i = j, \\
        0          & \text{otherwise}.
    \end{cases}
    $$

    Therefore,

    $$C_X = \begin{bmatrix}
        \sigma_1^2 & 0 & 0 & \cdots & 0 \\
        0 & \sigma_2^2 & 0 & \cdots & 0 \\
        0 & 0 & \sigma_3^2 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & 0 \\
        0 & 0 & 0 & \cdots & \sigma_n^2
    \end{bmatrix}
    $$

    \item [8.5.1]

    \begin{enumerate}[label=(\alph*)]
        \item

        \begin{align*}
        R_X & = C_X + \mu_X \mu_X' \\
            & = \begin{bmatrix}
                4 & -2 & 1 \\
                -2 & 4 & -2 \\
                1 & -2 & 4
                \end{bmatrix} + 
                \begin{bmatrix} 4 \\ 8 \\ 6 \end{bmatrix}
                \begin{bmatrix} 4 & 8 & 6 \end{bmatrix} \\
            & = \begin{bmatrix}
                4 & -2 & 1 \\
                -2 & 4 & -2 \\
                1 & -2 & 4
                \end{bmatrix} + 
                \begin{bmatrix}
                16 & 32 & 24 \\
                32 & 64 & 48 \\         
                24 & 48 & 36
                \end{bmatrix} \\
            & = \begin{bmatrix}
                20 & 30 & 25 \\
                30 & 68 & 46 \\
                25 & 46 & 40
                \end{bmatrix}.
        \end{align*}

        \item

        Let $Y = \begin{bmatrix} X_1 & X_2 \end{bmatrix}'$. Since $Y$ is a subset of the components of $X$, it is a Gaussian random vector.

        The expected value vector is,

        $$\mu_Y = \begin{bmatrix} \text E[X_1] \\ \text E[X_2] \end{bmatrix} = \begin{bmatrix} 4 \\ 8 \end{bmatrix}.$$

        The convariance matrix is,

        $$C_Y = \begin{bmatrix} \text{Var}[X_1] & \text{Cov}[X_1, X_2] \\ C_{X_1}X_2 & \text{Var}[X_2] \end{bmatrix} = \begin{bmatrix} 4 & -2 \\ -2 & 4 \end{bmatrix}.$$

        $$\text{det}(C_Y) = 16 - 4 = 12.$$

        The inverse of $C_Y$ is,

        $$C_Y^{-1} = \frac{1}{12} \begin{bmatrix} 4 & 2 \\ 2 & 4 \end{bmatrix} = \begin{bmatrix} \frac{1}{3} & \frac{1}{6} \\ \frac{1}{6} & \frac{1}{3} \end{bmatrix}.$$

        Thus,

        \begin{align*}
        (y - \mu_Y)'C_Y^{-1}(y - \mu_Y)
            & = \begin{bmatrix} y_1 - 4 & y_2 - 8 \end{bmatrix} \begin{bmatrix} 1 / 3 & 1 / 6 \\ 1 / 6 & 1 / 3 \end{bmatrix} \begin{bmatrix} y_1 - 4 \\ y_2 - 8 \end{bmatrix} \\
            & = \frac{y_1^2}{3} + \frac{y_1y_2}{3} - \frac{16y_1}{3} - \frac{20y_2}{3} + \frac{y_2^2}{3} + \frac{112}{3}.
        \end{align*}

        The PDF of $Y$ is,

        \begin{align*}
        f_Y(y)
        & = f_{X_1, X_2}(x_1, x_2) \\
        & = \frac{1}{(2\pi)^{n / 2}(\text{det}(C_X))^{1 / 2}} e^{-\frac{(y - \mu_Y)C_Y^{-1}(y - \mu_Y)}{2}} \\
        & = \frac{1}{4\sqrt 3 \pi} e^{-\frac{y_1^2 + y_1y_2 - 16y_1 - 20y_2 + y_2^2 + 112}{6}}.
        \end{align*}
            
        \item From the result of (b), the random variable $X_1$ is a Gaussian random variable with mean 4 and standard deviation 2.

        Thus,

        \begin{align*}
        \text P[X_1 > 8] 
            & = 1 - \text P \Bigg[ \frac{X_1 - 4}{2} < \frac{8 - 4}{2} \Bigg] \\
            & = 1 - \Phi(2) \\
            & = 1 - 0.97725 \\
            & = 0.0228.
        \end{align*}
    \end{enumerate}

    \item [9.1.3]

    \begin{enumerate}[label=(\alph*)]
        \item
        The PMF of $N_1$, the number of phone calls needed to obtain the correct answer, can be determined by observing that if the correct answer is given on the $n$th call, then the previous $n - 1$ calls must have given wrong answers so that

        $$
        P_{N_1}(n) = \begin{cases}
            (3 / 4)^{n - 1}(1 / 4) & n = 1, 2, \dots, \\
            0                      & \text{otherwise}.
        \end{cases}
        $$

        \item $E[N_1] = \frac{1}{p} = 4$.
        \item Like (a),

        $$
        P_{N_4}(n_4) = \begin{cases}
            \binom{n - 1}{3}(3 / 4)^{n - 4}(1 / 4)^4 & n = 4, 5, \dots, \\
            0                      & \text{otherwise}.
        \end{cases}
        $$

        \item By hint, $\text E[N_4] = 4 \text E[N_1] = 16$.

    \end{enumerate}

    \item [9.2.2]

    \begin{enumerate}[label=(\alph*)]
        \item

        $$P_J(j) = \begin{cases}
            0.6 & j = -2 \\
            0.4 & j = -1.
        \end{cases}
        $$

        The MGF of $J$ is
        
        \begin{align*}
        \phi_J(s) 
            & = \text E[e^{Js}] \\
            & = \sum_j e^{js} P_J(j) \\
            & = 0.6 e^{-2s} + 0.4e^{-s}.
        \end{align*}
        
        \item

        $$P_K(k) = \begin{cases}
            0.7 & k = -1 \\
            0.2 & k = 0 \\
            0.1 & k = 1.
        \end{cases}
        $$

        The MGF of $K$ is
        
        \begin{align*}
        \phi_K(s) 
            & = \text E[e^{Ks}] \\
            & = \sum_j e^{js} P_K(k) \\
            & = 0.7 e^{-s} + 0.2 + 0.1e^s.
        \end{align*}

        \item

        $$P_M(m) = \begin{cases}
            0.42 & m = -3, \\
            0.40 & m = -2, \\
            0.14 & m = -1, \\
            0.04 & m = 0.
        \end{cases}
        $$

        \item

        \begin{align*}
            \text E[M^4]
            & = \sum_m m^4 P_M(m) \\
            & = (-3)^4 \cdot 0.42 + (-2)^4 \cdot 0.40 + (-1)^4 \cdot 0.14 + 0 \\
            & = 40.56.
        \end{align*}
    
    \end{enumerate}

    \item [10.1.1]

    \begin{enumerate}[label=(\alph*)]
        \item

        By $\sigma_{Mn(x)}^2 = \sigma_X^2 / n$. Realizing that $\sigma_X^2 = 25$, we obtain

        $$\text{Var}[M_9(X)] = \frac{\sigma_X^2}{9} = \frac{25}{9}.$$

        \item

        \begin{align*}
        \text P[X_1 \ge 7]
            & = 1 - \text P[X_1 \le 7] \\
            & = 1 - F_X(7) \\
            & = 1 - (1 - e^{-7 / 5}) \\
            & = e^{-7 / 5} \approx 0.247.
        \end{align*}

        \item

        First we express $\text P[M_9(X) > 7]$ in terms of $X_1, \dots, X_9$.

        \begin{align*}
        \text P[M_9(X) > 7] & = 1 - \text P[M_9(X) \le 7] \\
        & = 1 - \text P[(X_1 + \cdots + X_9) \le 63].
        \end{align*}

        Now the probability that $M_9(X) > 7$ can be approximated using the Central Limit Theorem (CLT).

        \begin{align*}
        \text P[M_9(X) > 7] & = 1 - \text P[(X_1 + \cdots + X_9) \le 63] \\
        & \approx 1 - \Phi\Bigg(\frac{63 - 9\mu_X}{\sqrt 9 \sigma_X}\Bigg) \\
        & = 1 - \Phi(6 / 5) \approx 0.1151.
        \end{align*}

    \end{enumerate}

    \item [10.2.1]

    $$\text P[|W - \text E[W]| \ge 200] \le \frac{\text{Var}[W]}{200^2} \le \frac{100^2}{200^2} = 0.25.$$


\end{enumerate}

\end{document}